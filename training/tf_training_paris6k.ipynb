{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Z_jqsWcdF4lb","PULQlqJTMRrL"],"authorship_tag":"ABX9TyNWwrWmcKvj6r4SMhEk+QZl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Monument Classification with Transfer Learning and TensorFlow Lite"],"metadata":{"id":"trNnIFVNIBpX"}},{"cell_type":"markdown","source":["1. Starting with a pre-trained model (in this case, MobileNetV2)\n","2. Fine-tuning it on paris6k\n","3. Converting the fine-tuned model to TensorFlow Lite format"],"metadata":{"id":"QS-vm_qFLvCC"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"_IcSP6xC--0g"}},{"cell_type":"markdown","source":["### Install required packages"],"metadata":{"id":"BM8YC5nkIE33"}},{"cell_type":"code","source":["!python --version\n","!pip install --upgrade pip\n","!pip install tensorflow\n","!pip install albumentations\n","!pip install pycocotools"],"metadata":{"id":"6hTPNnZp_DiG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import necessary libraries"],"metadata":{"id":"Mo-UEKdOIQXJ"}},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","import json\n","import tensorflow as tf\n","import albumentations as A\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pycocotools.coco import COCO\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","\n","#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","#import cv2\n","\n","assert tf.__version__.startswith('2')"],"metadata":{"id":"I1rxFMi6_A5S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Mount Google Drive and Set Paths"],"metadata":{"id":"Vut4cGcH-3m-"}},{"cell_type":"markdown","source":["### Mount Google Drive"],"metadata":{"id":"Uwiszy6EIeEH"}},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"T_gfxE-_CFp_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define paths"],"metadata":{"id":"dqivQXQDIfYd"}},{"cell_type":"code","source":["base_path = '/content/drive/MyDrive/'\n","source_path = base_path + 'Datasets/revisitop/rparis6k/data/'\n","dest_base_path = base_path + 'MyProject/rparis6k/'\n","\n","train_dataset_path = dest_base_path + 'train/'\n","validation_dataset_path = dest_base_path + 'validation/'\n","test_dataset_path = dest_base_path + 'test/'"],"metadata":{"id":"0SmkSGAvB-5h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset Preparation"],"metadata":{"id":"jPMWHe1BIjE6"}},{"cell_type":"markdown","source":["### Copy images"],"metadata":{"id":"F41txtrnFxe_"}},{"cell_type":"code","source":["# Function to copy images\n","def copy_images(file_list, dest_folder):\n","    with open(file_list, 'r') as f:\n","        for line in f:\n","            img_name = line.strip()\n","            src = os.path.join(source_path, img_name)\n","            dst = os.path.join(dest_folder, img_name)\n","            os.makedirs(os.path.dirname(dst), exist_ok=True)\n","            shutil.copy2(src, dst)\n","\n","# Copy images for each set\n","if not os.listdir(train_dataset_path) and not os.listdir(validation_dataset_path) and not os.listdir(test_dataset_path):\n","    copy_images(dest_base_path + 'train.txt', train_dataset_path + 'images/')\n","    copy_images(dest_base_path + 'val.txt', validation_dataset_path + 'images/')\n","    copy_images(dest_base_path + 'test.txt', test_dataset_path + 'images/')\n","    print(\"Dataset division completed!\\n\")\n","else:\n","    print(\"One or more directories are not empty. Copy operation aborted.\\n\")\n","\n","print(f\"Number of images in train set: {len(os.listdir(train_dataset_path + 'images/'))}\")\n","print(f\"Number of images in validation set: {len(os.listdir(validation_dataset_path + 'images/'))}\")\n","print(f\"Number of images in test set: {len(os.listdir(test_dataset_path + 'images/'))}\")"],"metadata":{"id":"Q0BXXZ71Fz5q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Review dataset"],"metadata":{"id":"Z_jqsWcdF4lb"}},{"cell_type":"code","source":["with open(os.path.join(train_dataset_path, \"labels.json\"), \"r\") as f:\n","  labels_json = json.load(f)\n","for category_item in labels_json[\"categories\"]:\n","  print(f\"{category_item['id']}: {category_item['name']}\")"],"metadata":{"id":"k0_pG4aaF7qS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Augmentation"],"metadata":{"id":"i2JaY6MqBvly"}},{"cell_type":"code","source":["# Define Albumentations transformations\n","train_transform = A.Compose([\n","    A.RandomRotate90(),\n","    A.Flip(),\n","    A.Transpose(),\n","    A.OneOf([\n","        A.IAAAdditiveGaussianNoise(),\n","        A.GaussNoise(),\n","    ], p=0.2),\n","    A.OneOf([\n","        A.MotionBlur(p=0.2),\n","        A.MedianBlur(blur_limit=3, p=0.1),\n","        A.Blur(blur_limit=3, p=0.1),\n","    ], p=0.2),\n","    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.5, rotate_limit=45, p=0.2),\n","    A.OneOf([\n","        A.OpticalDistortion(p=0.3),\n","        A.GridDistortion(p=0.1),\n","        A.IAAPiecewiseAffine(p=0.3),\n","    ], p=0.2),\n","    A.OneOf([\n","        A.CLAHE(clip_limit=2),\n","        A.IAASharpen(),\n","        A.IAAEmboss(),\n","        A.RandomBrightnessContrast(),\n","    ], p=0.3),\n","    A.HueSaturationValue(p=0.3),\n","])\n","\n","def augment_image(image, transform=train_transform):\n","    image = np.array(image)\n","    augmented = transform(image=image)\n","    return augmented['image']\n"],"metadata":{"id":"BRlaPD3-BxlI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset Creation"],"metadata":{"id":"idjelgoRB0U6"}},{"cell_type":"code","source":["def load_and_preprocess_image(path):\n","    image = tf.io.read_file(path)\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = tf.image.resize(image, [224, 224])\n","    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1]\n","    return image\n","\n","def preprocess_and_augment(image, label):\n","    image = tf.numpy_function(augment_image, [image], tf.float32)\n","    image.set_shape([224, 224, 3])\n","    return image, label\n","\n","def get_dataset(image_paths, labels, batch_size, is_training=False):\n","    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n","    dataset = dataset.map(lambda x, y: (load_and_preprocess_image(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n","    if is_training:\n","        dataset = dataset.map(preprocess_and_augment, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.shuffle(buffer_size=1000)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    return dataset"],"metadata":{"id":"dyzcJs04E1Fe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the dataset"],"metadata":{"id":"irw8zVStJpwg"}},{"cell_type":"code","source":["image_paths, labels = load_your_dataset()  # TODO: implement this function"],"metadata":{"id":"EeedjGy7JpMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This function should return two lists: one containing the paths to the images,\n","# and another containing the corresponding labels (as one-hot encoded vectors)."],"metadata":{"id":"qQLiCyCPMgwP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Split the dataset"],"metadata":{"id":"uKY0g7GEJx5W"}},{"cell_type":"code","source":["# Split the dataset\n","split_index = int(len(image_paths) * 0.8)\n","image_paths_train = image_paths[:split_index]\n","labels_train = labels[:split_index]\n","image_paths_val = image_paths[split_index:]\n","labels_val = labels[split_index:]\n","\n","train_dataset = get_dataset(image_paths_train, labels_train, batch_size=32, is_training=True)\n","val_dataset = get_dataset(image_paths_val, labels_val, batch_size=32, is_training=False)"],"metadata":{"id":"fSrzwTK9J1x-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"mHbrAGItKJEW"}},{"cell_type":"markdown","source":["### Model creations"],"metadata":{"id":"1q-vKKrm_OoU"}},{"cell_type":"code","source":["# Load pre-trained MobileNetV2 model\n","base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# Freeze base model layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Add custom layers\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","predictions = Dense(13, activation='softmax')(x)  # 13 classes (12 monuments + background)\n","\n","model = Model(inputs=base_model.input, outputs=predictions)"],"metadata":{"id":"ogWWkKa2_QAu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model compilation"],"metadata":{"id":"O5bGdIfAKESs"}},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"collapsed":true,"id":"MTDMWS1m_RU8","executionInfo":{"status":"error","timestamp":1720637611502,"user_tz":-120,"elapsed":381,"user":{"displayName":"Elia Innocenti","userId":"07866908462894922277"}},"outputId":"2af75516-14fd-4d40-87a1-d35bc36a34f1"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-228481410b08>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"markdown","source":["### Fine-tuning"],"metadata":{"id":"G1WUgy_5M7j0"}},{"cell_type":"code","source":["# Unfreeze the top layers of the base model\n","for layer in base_model.layers[-20:]:\n","    layer.trainable = True\n","\n","# Recompile the model with a lower learning rate\n","model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Continue training\n","model.fit(...)"],"metadata":{"id":"xQkqRvtnM9fU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"ZbPYOlUDKYKb"}},{"cell_type":"markdown","source":["- Start by training for a few epochs and monitor the validation loss and accuracy.\n","- If the model is underfitting (high training and validation loss):\n","  1. Unfreeze more layers of the base model\n","  2. Train for more epochs\n","  3. Increase model capacity (add more dense layers)\n","- If the model is overfitting (low training loss, high validation loss):\n","  1. Add regularization (e.g., dropout layers)\n","  2. Use data augmentation (already implemented)\n","  3.  Reduce model capacity"],"metadata":{"id":"NqvcuM_AL85p"}},{"cell_type":"markdown","source":["- Start with a small number of epochs (e.g., 10) and monitor the training and validation metrics.\n","- Gradually increase the number of epochs if needed.\n","- Use the ModelCheckpoint callback to save the best model based on validation accuracy.\n","- Use the EarlyStopping callback to prevent overfitting by stopping training when the validation loss stops improving."],"metadata":{"id":"1cspPobzMmuU"}},{"cell_type":"markdown","source":["### Resume training"],"metadata":{"id":"PULQlqJTMRrL"}},{"cell_type":"code","source":["# Load the saved model\n","model = tf.keras.models.load_model('best_model.h5')\n","\n","# Continue training\n","model.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=10,\n","    initial_epoch=history.epoch[-1],  # Start from the last epoch\n","    callbacks=[\n","        tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy'),\n","        tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')\n","    ]\n",")"],"metadata":{"id":"jFmlaG9lMZ4q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the model"],"metadata":{"id":"0HAl3H3SKgGF"}},{"cell_type":"code","source":["history = model.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=10,\n","    callbacks=[\n","        tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy'),\n","        tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')\n","    ]\n",")"],"metadata":{"id":"L0Jije_c_TWh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate"],"metadata":{"id":"dWwHp7oj_a_k"}},{"cell_type":"markdown","source":["### Evaluate the model"],"metadata":{"id":"GIicAZ6vKodk"}},{"cell_type":"code","source":["loss, accuracy = model.evaluate(val_dataset)\n","print(f\"Validation loss: {loss}\")\n","print(f\"Validation accuracy: {accuracy}\")"],"metadata":{"id":"lwjCx0Ir_aqt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Export to TensorFlow Lite"],"metadata":{"id":"4tArnfxu_ceT"}},{"cell_type":"markdown","source":["### Convert to TFLite"],"metadata":{"id":"U2agEoARKxJ6"}},{"cell_type":"code","source":["model.save('monumenti_model.h5') # TODO: check line\n","\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()"],"metadata":{"id":"1RwFPHZU_e3c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save the TFLite model"],"metadata":{"id":"jCwfjbWkKycO"}},{"cell_type":"code","source":["with open('monuments_model.tflite', 'wb') as f:\n","    f.write(tflite_model)"],"metadata":{"id":"KoNhYZ5YK1Pn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quantization"],"metadata":{"id":"ERb5D4Uc_fIb"}},{"cell_type":"markdown","source":["### Quantize the model"],"metadata":{"id":"XBQPKRTlLEUU"}},{"cell_type":"code","source":["converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","tflite_model_quantized = converter.convert()"],"metadata":{"id":"sMDtUnD7_gx2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save the quantized TFLite model"],"metadata":{"id":"JNh-G1ZVLF1n"}},{"cell_type":"code","source":["with open('monuments_model_quantized.tflite', 'wb') as f:\n","    f.write(tflite_model_quantized)"],"metadata":{"id":"lYl3k1x1LIyM"},"execution_count":null,"outputs":[]}]}